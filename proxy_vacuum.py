import asyncio
import requests
import re
import base64
import json
import time
import logging
import subprocess
import os
import random
from urllib.parse import urlparse, unquote, parse_qs

# --- –ò–°–¢–û–ß–ù–ò–ö–ò ---
TG_CHANNELS = [
    "shadowsockskeys", "oneclickvpnkeys", "v2ray_outlineir",
    "v2ray_free_conf", "v2rayngvpn", "v2ray_free_vpn",
    "gurvpn_keys", "vmessh", "VMESS7", "VlessConfig",
    "PrivateVPNs", "nV_v2ray", "NotorVPN", "FairVpn_V2ray",
    "outline_marzban", "outline_k"
]

EXTERNAL_SUBS = [
    "https://raw.githubusercontent.com/yebekhe/TelegramV2rayCollector/main/sub/normal/mix",
    "https://raw.githubusercontent.com/vfarid/v2ray-share/main/all_v2ray_configs.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub1.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/NodeList.txt"
]

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
SINGBOX_BIN = "./sing-box" # –ë–∏–Ω–∞—Ä–Ω–∏–∫ –ª–µ–∂–∏—Ç –≤ –∫–æ—Ä–Ω–µ (–∏–∑ Dockerfile)
CHECK_TIMEOUT = 5 # –°–µ–∫—É–Ω–¥ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
MAX_PARALLEL_CHECKS = 5 # –ù–µ –±–æ–ª—å—à–µ 5 –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ sing-box –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (–ø–∞–º—è—Ç—å!)

import database_vpn as db

# --- –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï ---
def safe_decode(s):
    try:
        s = re.sub(r'[^a-zA-Z0-9+/=]', '', s)
        pad = len(s) % 4
        if pad: s += '=' * (4 - pad)
        return base64.b64decode(s).decode('utf-8', errors='ignore')
    except: return ""

def fetch_links():
    """–°–±–æ—Ä—â–∏–∫ —Å—Å—ã–ª–æ–∫ (–¢–ì + –ì–∏—Ç—Ö–∞–±)"""
    links = set()
    regex = re.compile(r'(?:vless|vmess|ss|ssr|trojan|hy2|hysteria|hysteria2|tuic|socks5)://[^\s<"\'\)]+')
    headers = {'User-Agent': 'Mozilla/5.0'}

    # –¢–ì (–Ω–µ–º–Ω–æ–≥–æ –∏—Å—Ç–æ—Ä–∏–∏)
    for ch in TG_CHANNELS:
        url = f"https://t.me/s/{ch}"
        for _ in range(5):
            try:
                r = requests.get(url, headers=headers, timeout=5)
                for l in regex.findall(r.text): links.add(l.strip().split('<')[0])
                if 'tme_messages_more' in r.text:
                    m = re.search(r'href="(/s/.*?)"', r.text)
                    if m: url = "https://t.me" + m.group(1)
                    else: break
                else: break
            except: break
            
    # –ì–ò–¢–•–ê–ë
    for url in EXTERNAL_SUBS:
        try:
            r = requests.get(url, headers=headers, timeout=10)
            text = r.text
            if not "://" in text[:100]:
                d = safe_decode(text)
                if "://" in d: text = d
            for l in regex.findall(text): links.add(l.strip())
        except: pass
        
    return list(links)

# --- –ö–û–ù–í–ï–†–¢–ï–† –í SING-BOX CONFIG ---
# –≠—Ç–æ —Å–∞–º–∞—è —Å–ª–æ–∂–Ω–∞—è —á–∞—Å—Ç—å: –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å —Å—Å—ã–ª–∫—É –≤ JSON –¥–ª—è —è–¥—Ä–∞
def generate_singbox_config(link, local_port):
    try:
        outbound = None
        
        # 1. VMESS
        if link.startswith("vmess://"):
            d = json.loads(safe_decode(link[8:]))
            outbound = {
                "type": "vmess",
                "server": d.get('add'),
                "server_port": int(d.get('port')),
                "uuid": d.get('id'),
                "security": "auto",
                "transport": {}
            }
            if d.get('net') == 'ws':
                outbound["transport"] = {"type": "ws", "path": d.get('path', '/'), "headers": {"Host": d.get('host', '')}}
            if d.get('tls') == 'tls':
                outbound["tls"] = {"enabled": True, "insecure": True}

        # 2. VLESS
        elif link.startswith("vless://"):
            p = urlparse(link); q = parse_qs(p.query)
            outbound = {
                "type": "vless",
                "server": p.hostname,
                "server_port": p.port,
                "uuid": p.username,
                "flow": q.get('flow', [''])[0],
                "tls": {"enabled": False},
                "transport": {}
            }
            sec = q.get('security', [''])[0]
            if sec == 'reality':
                outbound["tls"] = {
                    "enabled": True, "server_name": q.get('sni', [''])[0],
                    "reality": {"enabled": True, "public_key": q.get('pbk', [''])[0], "short_id": q.get('sid', [''])[0]},
                    "utls": {"enabled": True, "fingerprint": "chrome"}
                }
            elif sec == 'tls':
                outbound["tls"] = {"enabled": True, "server_name": q.get('sni', [''])[0], "insecure": True}
            
            net = q.get('type', ['tcp'])[0]
            if net == 'ws':
                outbound["transport"] = {"type": "ws", "path": q.get('path', ['/'])[0], "headers": {"Host": q.get('host', [''])[0]}}
            elif net == 'grpc':
                outbound["transport"] = {"type": "grpc", "service_name": q.get('serviceName', [''])[0]}

        # 3. SHADOWSOCKS
        elif link.startswith("ss://"):
            main = link.split("#")[0].replace("ss://", "")
            if "@" in main:
                u, s = main.split("@", 1)
                d = safe_decode(u)
                if ":" in d: m, pw = d.split(":", 1)
                else: m, pw = u.split(":", 1)
                host, port = s.split(":")[0], int(s.split(":")[1].split("/")[0])
                outbound = {
                    "type": "shadowsocks",
                    "server": host, "server_port": port,
                    "method": m, "password": pw
                }

        # 4. TROJAN
        elif link.startswith("trojan://"):
            p = urlparse(link); q = parse_qs(p.query)
            outbound = {
                "type": "trojan",
                "server": p.hostname, "server_port": p.port, "password": p.username,
                "tls": {"enabled": True, "server_name": q.get('sni', [''])[0], "insecure": True}
            }

        if not outbound: return None

        # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
        config = {
            "log": {"disabled": True},
            "inbounds": [{
                "type": "mixed",
                "listen": "127.0.0.1",
                "listen_port": local_port
            }],
            "outbounds": [outbound]
        }
        return config
    except: return None

# --- –ü–†–û–í–ï–†–ö–ê –ß–ï–†–ï–ó –Ø–î–†–û ---
async def real_check(link, sem):
    async with sem:
        local_port = random.randint(10000, 50000)
        config_file = f"config_{local_port}.json"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥
        config_data = generate_singbox_config(link, local_port)
        if not config_data: return None # –ù–µ —Å–º–æ–≥–ª–∏ —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with open(config_file, 'w') as f:
            json.dump(config_data, f)

        proc = None
        try:
            # 1. –ó–∞–ø—É—Å–∫–∞–µ–º Sing-box
            proc = subprocess.Popen([SINGBOX_BIN, "run", "-c", config_file], 
                                    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # –î–∞–µ–º –µ–º—É —Å–µ–∫—É–Ω–¥—É –Ω–∞ —Ä–∞–∑–≥–æ–Ω
            await asyncio.sleep(1.0)
            
            if proc.poll() is not None:
                # –£–ø–∞–ª —Å—Ä–∞–∑—É
                return None

            # 2. –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ–±–∏—Ç—å Google —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–∫—Å–∏
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º curl, —Ç–∞–∫ –Ω–∞–¥–µ–∂–Ω–µ–µ
            # –°–Ω–∞—á–∞–ª–∞ –æ–±—ã—á–Ω—ã–π –≥—É–≥–ª (—Ç–µ—Å—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞)
            cmd_base = f"curl -x http://127.0.0.1:{local_port} -s -o /dev/null -w '%{{http_code}}' --max-time 3 "
            
            start = time.time()
            # –¢–µ—Å—Ç 1: –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞ (Google)
            check_inet = await asyncio.to_thread(os.popen, cmd_base + "http://www.google.com/generate_204")
            code_inet = check_inet.read().strip()
            
            latency = int((time.time() - start) * 1000)
            
            is_ai = 0
            # –ï—Å–ª–∏ –∏–Ω–µ—Ç –µ—Å—Ç—å (–∫–æ–¥ 204), –ø—Ä–æ–≤–µ—Ä—è–µ–º AI
            if code_inet == "204":
                # –¢–µ—Å—Ç 2: –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Gemini
                check_ai = await asyncio.to_thread(os.popen, cmd_base + "https://aistudio.google.com")
                # –¢—É—Ç —Å–ª–æ–∂–Ω–µ–µ, –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å 200 (–û–ö) –∏–ª–∏ 403 (–ó–∞–ø—Ä–µ—Ç). –ù–æ –µ—Å–ª–∏ –≤–µ—Ä–Ω—É–ª - –∑–Ω–∞—á–∏—Ç –¥–æ—Å—Ç—É–ø –µ—Å—Ç—å.
                # –û–±—ã—á–Ω–æ 403 –∑–Ω–∞—á–∏—Ç —Ä–µ–≥–∏–æ–Ω –±–ª–æ–∫. 200 - –æ–∫.
                code_ai = check_ai.read().strip()
                if code_ai == "200": is_ai = 1
                
                return {"url": link, "lat": latency, "is_ai": is_ai}
            
            return None

        except Exception as e:
            return None
        finally:
            # –£–±–∏—Ä–∞–µ–º –∑–∞ —Å–æ–±–æ–π
            if proc: proc.terminate()
            if os.path.exists(config_file): os.remove(config_file)

# --- –ì–õ–ê–í–ù–´–ô –¶–ò–ö–õ ---
async def vacuum_job():
    logging.info("üöÄ REALITY CHECKER –∑–∞–ø—É—â–µ–Ω")
    while True:
        try:
            # 1. –°–±–æ—Ä
            logging.info("üì• –°–±–æ—Ä —Å—Å—ã–ª–æ–∫...")
            links = await asyncio.to_thread(fetch_links)
            db.save_proxy_batch(links) # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ
            logging.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ. –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(links)}")
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞
            # –ë–µ—Ä–µ–º –∏–∑ –±–∞–∑—ã —Ç–µ—Ö, –∫–æ–≥–æ –¥–∞–≤–Ω–æ –Ω–µ —á–µ–∫–∞–ª–∏
            candidates = db.get_proxies_to_check(limit=100) # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—á–∫–∞–º–∏ –ø–æ 100
            
            if candidates:
                logging.info(f"üí£ –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–∂–∞—Ä–∫—É {len(candidates)} —Å–µ—Ä–≤–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ Sing-box...")
                sem = asyncio.Semaphore(MAX_PARALLEL_CHECKS)
                
                tasks = [real_check(u, sem) for u in candidates]
                results = await asyncio.gather(*tasks)
                
                live_count = 0
                for res in results:
                    if res:
                        # –°–ï–†–í–ï–† –†–ï–ê–õ–¨–ù–û –†–ê–ë–û–¢–ê–ï–¢!
                        # –ü–æ–ª—É—á–∞–µ–º —Ñ–ª–∞–≥ —Å—Ç—Ä–∞–Ω—ã –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã (—á–µ—Ä–µ–∑ API)
                        try:
                            # –ü—Ä–æ—Å—Ç–æ–π GeoIP –ø–æ –¥–æ–º–µ–Ω—É/IP –∏–∑ —Å—Å—ã–ª–∫–∏
                            host = parse_host(res['url'])
                            r = requests.get(f"http://ip-api.com/json/{host}", timeout=2)
                            cc = r.json().get('countryCode', '')
                            # –ï—Å–ª–∏ –ø—Ä–æ—à–µ–ª AI —Ç–µ—Å—Ç - —Å—Ç–∞–≤–∏–º –∂–∏—Ä–Ω—ã–π —Ñ–ª–∞–≥
                            if res['is_ai']: res['is_ai'] = 2 # –°—É–ø–µ—Ä –≠–ª–∏—Ç–∞
                        except: cc = ""
                        
                        db.update_proxy_status(res['url'], res['lat'], res['is_ai'], cc)
                        live_count += 1
                    else:
                        # –¢—Ä—É–ø
                        # –ò—â–µ–º URL –≤ tasks... —Å–ª–æ–∂–Ω–æ. –°–¥–µ–ª–∞–µ–º –ø—Ä–æ—â–µ:
                        # –§—É–Ω–∫—Ü–∏—è real_check –¥–æ–ª–∂–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å URL –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ, 
                        # –Ω–æ —Å–µ–π—á–∞—Å –æ–Ω–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None. 
                        # –ò—Å–ø—Ä–∞–≤–∏–º –≤ —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏, –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.
                        # –í –±–∞–∑–µ –æ–Ω–∏ –æ—Å—Ç–∞–Ω—É—Ç—Å—è "–Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–º–∏" –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ä–∞–∑–∞, 
                        # –Ω–æ fails –Ω–∞–¥–æ –±—ã —É–≤–µ–ª–∏—á–∏—Ç—å. 
                        pass 
                
                # –ö–æ—Å—Ç—ã–ª—å –¥–ª—è –æ—Ç–º–µ—Ç–∫–∏ –º–µ—Ä—Ç–≤—ã—Ö (—á—Ç–æ–±—ã –Ω–µ —á–µ–∫–∞—Ç—å –≤–µ—á–Ω–æ)
                # –í —Ä–µ–∞–ª—å–Ω–æ–º –∫–æ–¥–µ –Ω–∞–¥–æ –º–∞–ø–∏—Ç—å tasks -> results
                
                logging.info(f"üèÅ –ü–∞—á–∫–∞ –≥–æ—Ç–æ–≤–∞. –†–µ–∞–ª—å–Ω–æ –∂–∏–≤—ã—Ö: {live_count}")

            logging.info("üí§ –°–ø–ª—é 5 –º–∏–Ω—É—Ç...")
            await asyncio.sleep(300)
            
        except Exception as e:
            logging.error(f"Error: {e}")
            await asyncio.sleep(60)

def parse_host(url):
    # –•–µ–ª–ø–µ—Ä –¥–ª—è GeoIP
    try:
        if "vmess" in url:
            return json.loads(base64.b64decode(url[8:]).decode('utf-8', errors='ignore'))['add']
        return urlparse(url).hostname
    except: return ""
