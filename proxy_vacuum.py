import asyncio
import requests
import re
import base64
import json
import time
import logging
from urllib.parse import urlparse, unquote, quote
import database as db

# --- –°–ü–ò–°–ö–ò –ò–°–¢–û–ß–ù–ò–ö–û–í ---
TG_CHANNELS = [
    "shadowsockskeys", "oneclickvpnkeys", "v2ray_outlineir",
    "v2ray_free_conf", "v2rayngvpn", "v2ray_free_vpn",
    "gurvpn_keys", "vmessh", "VMESS7", "VlessConfig",
    "PrivateVPNs", "nV_v2ray", "NotorVPN", "FairVpn_V2ray",
    "outline_marzban", "outline_k"
]

EXTERNAL_SUBS = [
    "https://raw.githubusercontent.com/yebekhe/TelegramV2rayCollector/main/sub/normal/mix",
    "https://raw.githubusercontent.com/vfarid/v2ray-share/main/all_v2ray_configs.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub1.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/NodeList.txt",
    "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge.txt"
]

# --- –§–£–ù–ö–¶–ò–ò –°–ë–û–†–ê ---
def safe_decode(s):
    try:
        s = re.sub(r'[^a-zA-Z0-9+/=]', '', s)
        padding = len(s) % 4
        if padding: s += '=' * (4 - padding)
        return base64.b64decode(s).decode('utf-8', errors='ignore')
    except: return ""

def scrape_everything():
    """–°–æ–±–∏—Ä–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –æ—Ç–æ–≤—Å—é–¥—É"""
    logging.info("üßπ Vacuum: –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫...")
    links = set()
    regex = re.compile(r'(?:vless|vmess|ss|ssr|trojan|hy2|hysteria|hysteria2|tuic|socks5)://[^\s<"\'\)]+')
    headers = {'User-Agent': 'Mozilla/5.0'}

    # 1. –¢–ï–õ–ï–ì–†–ê–ú (–ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 –ø–æ—Å—Ç–æ–≤, –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –ª–∏—Å—Ç–∞–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å)
    for ch in TG_CHANNELS:
        try:
            r = requests.get(f"https://t.me/s/{ch}", headers=headers, timeout=5)
            found = regex.findall(r.text)
            for l in found:
                clean = l.strip().split('<')[0].split('"')[0]
                links.add(clean)
        except: pass

    # 2. –ì–ò–¢–•–ê–ë
    for url in EXTERNAL_SUBS:
        try:
            r = requests.get(url, headers=headers, timeout=10)
            text = r.text
            # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
            decoded = safe_decode(text)
            if len(decoded) > 100: text = decoded
            
            found = regex.findall(text)
            for l in found:
                clean = l.strip()
                links.add(clean)
        except: pass
    
    return list(links)

# --- –§–£–ù–ö–¶–ò–ò –ü–†–û–í–ï–†–ö–ò ---
def extract_ip_port(link):
    try:
        if link.startswith("vmess://"):
            data = json.loads(safe_decode(link[8:]))
            return data.get('add'), int(data.get('port'))
        p = urlparse(link)
        if link.startswith("ss://") and "@" in link:
            part = link.split("@")[-1].split("#")[0].split("/")[0]
            if ":" in part: 
                return part.split(":")[0].replace("[","").replace("]",""), int(part.split(":")[1])
        if p.hostname and p.port: return p.hostname, p.port
    except: pass
    return None, None

async def check_connectivity(ip, port):
    """
    –õ–µ–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ TCP.
    –ï—Å–ª–∏ –ø–æ—Ä—Ç –æ—Ç–∫—Ä—ã—Ç –∏ –æ—Ç–≤–µ—á–∞–µ—Ç –±—ã—Å—Ç—Ä–æ - —Å—á–∏—Ç–∞–µ–º —Å–µ—Ä–≤–µ—Ä –∂–∏–≤—ã–º.
    –î–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞ —ç—Ç–æ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ.
    """
    try:
        start = time.time()
        conn = asyncio.open_connection(ip, port)
        _, writer = await asyncio.wait_for(conn, timeout=1.5) # –¢–∞–π–º-–∞—É—Ç 1.5 —Å–µ–∫
        latency = int((time.time() - start) * 1000)
        writer.close()
        await writer.wait_closed()
        
        # –û—Ç—Å–µ–∫–∞–µ–º —Ñ–µ–π–∫–∏ < 5–º—Å
        if latency < 5: return None
        return latency
    except:
        return None

def get_geo_info(ip):
    # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π GeoIP (–æ–¥–∏–Ω–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å, —á—Ç–æ–±—ã –Ω–µ –±–∞–Ω–∏–ª–∏ –±–∞—Ç—á–∞–º–∏)
    # –ú–æ–∂–Ω–æ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞—Ç—å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–∑—É, –Ω–æ –ø–æ–∫–∞ —Ç–∞–∫
    return "üè≥Ô∏è" # –ü–æ–∫–∞ –∑–∞–≥–ª—É—à–∫–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

# --- –ì–õ–ê–í–ù–´–ô –¶–ò–ö–õ ---
async def start_vacuum():
    while True:
        try:
            # 1. –°–±–æ—Ä
            all_links = scrape_everything()
            added = db.save_proxy_batch(all_links)
            logging.info(f"üßπ Vacuum: –î–æ–±–∞–≤–ª–µ–Ω–æ {added} –Ω–æ–≤—ã—Ö. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_links)}")
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ (–ë–µ—Ä–µ–º –∏–∑ –±–∞–∑—ã –ø–∞—á–∫—É –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö)
            candidates = db.get_proxies_to_check(limit=100) # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ 100 —à—Ç—É–∫ –∑–∞ —Ä–∞–∑
            logging.info(f"üß™ Vacuum: –ü—Ä–æ–≤–µ—Ä—è—é {len(candidates)} –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")
            
            tasks = []
            for link in candidates:
                ip, port = extract_ip_port(link)
                if ip and port:
                    tasks.append((link, ip, port))
                else:
                    db.update_proxy_status(link, None, 0, "") # –ù–µ–≤–∞–ª–∏–¥
            
            for link, ip, port in tasks:
                lat = await check_connectivity(ip, port)
                # –¢—É—Ç –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–∞ AI (–ø–æ–∫–∞ —Ä–∞–Ω–¥–æ–º –∏–ª–∏ –∑–∞–≥–ª—É—à–∫–∞)
                is_ai = 1 if lat and lat < 200 else 0 # –ü—Ä–∏–º–µ—Ä: –±—ã—Å—Ç—Ä—ã–µ —Å—á–∏—Ç–∞–µ–º AI
                country = "" 
                
                db.update_proxy_status(link, lat, is_ai, country)
                
            logging.info("üí§ Vacuum: –°–ø–ª—é 10 –º–∏–Ω—É—Ç...")
            await asyncio.sleep(600) # –ü–∞—É–∑–∞ 10 –º–∏–Ω
            
        except Exception as e:
            logging.error(f"Vacuum Error: {e}")
            await asyncio.sleep(60)
