import asyncio
import requests
import re
import base64
import json
import time
import logging
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import database_vpn as db

# --- –°–ü–ò–°–ö–ò –ò–°–¢–û–ß–ù–ò–ö–û–í ---
TG_CHANNELS = [
    "shadowsockskeys", "oneclickvpnkeys", "v2ray_outlineir",
    "v2ray_free_conf", "v2rayngvpn", "v2ray_free_vpn",
    "gurvpn_keys", "vmessh", "VMESS7", "VlessConfig",
    "PrivateVPNs", "nV_v2ray", "NotorVPN", "FairVpn_V2ray",
    "outline_marzban", "outline_k"
]

EXTERNAL_SUBS = [
    "https://raw.githubusercontent.com/yebekhe/TelegramV2rayCollector/main/sub/normal/mix",
    "https://raw.githubusercontent.com/vfarid/v2ray-share/main/all_v2ray_configs.txt",
    "https://raw.githubusercontent.com/barry-far/V2ray-Configs/main/Sub1.txt",
    "https://raw.githubusercontent.com/mahdibland/V2RayAggregator/master/sub/sub_merge.txt",
    "https://raw.githubusercontent.com/LonUp/NodeList/main/NodeList.txt",
    "https://raw.githubusercontent.com/officialputuid/V2Ray-Config/main/Splitted-v2ray-config/all"
]

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—ã–ª–µ—Å–æ—Å–∞
MAX_PAGES_TG = 1000  # –°–∫–æ–ª—å–∫–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏—Å—Ç–æ—Ä–∏–∏ –ª–∏—Å—Ç–∞—Ç—å –Ω–∞–∑–∞–¥ (–≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫)
MAX_LINKS_CHECK = 200 # –°–∫–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –∑–∞ –æ–¥–∏–Ω —Ü–∏–∫–ª (—á—Ç–æ–±—ã –Ω–µ –∑–∞–±–∏—Ç—å –ø–∞–º—è—Ç—å)

def safe_decode(s):
    try:
        s = re.sub(r'[^a-zA-Z0-9+/=]', '', s)
        padding = len(s) % 4
        if padding: s += '=' * (4 - padding)
        return base64.b64decode(s).decode('utf-8', errors='ignore')
    except: return ""

def scrape_sync():
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —á–∞—Å—Ç—å —Å–±–æ—Ä–∞ (—á—Ç–æ–±—ã –Ω–µ –≤–µ—à–∞—Ç—å –±–æ—Ç–∞, –∑–∞–ø—É—Å—Ç–∏–º –≤ —Ç—Ä–µ–¥–µ)"""
    links = set()
    regex = re.compile(r'(?:vless|vmess|ss|ssr|trojan|hy2|hysteria|hysteria2|tuic|socks5)://[^\s<"\'\)]+')
    headers = {'User-Agent': 'Mozilla/5.0'}

    logging.info("üßπ Vacuum: –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä —Å –ì–∏—Ç—Ö–∞–±–∞...")
    for url in EXTERNAL_SUBS:
        try:
            r = requests.get(url, headers=headers, timeout=15)
            text = r.text
            # –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å
            if len(text) > 10 and not "://" in text[:50]:
                decoded = safe_decode(text)
                if decoded: text = decoded
            
            found = regex.findall(text)
            for l in found: links.add(l.strip())
        except: pass

    logging.info(f"üßπ Vacuum: –ì–∏—Ç—Ö–∞–± –¥–∞–ª {len(links)}. –ò–¥—É –≤ –¢–µ–ª–µ–≥—Ä–∞–º (–ì–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫)...")
    
    for ch in TG_CHANNELS:
        url = f"https://t.me/s/{ch}"
        pages = 0
        try:
            while pages < MAX_PAGES_TG:
                r = requests.get(url, headers=headers, timeout=10)
                soup = BeautifulSoup(r.text, 'html.parser')
                msgs = soup.find_all('div', class_='tgme_widget_message_text')
                
                if not msgs: break
                
                # –°–æ–±–∏—Ä–∞–µ–º —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                found_on_page = 0
                for m in msgs:
                    found = regex.findall(m.get_text())
                    for l in found:
                        clean = l.strip().split('<')[0].split('"')[0]
                        links.add(clean)
                        found_on_page += 1
                
                # –ò—â–µ–º –∫–Ω–æ–ø–∫—É "More" (—Å—Ç–∞—Ä—ã–µ –ø–æ—Å—Ç—ã)
                more = soup.find('a', class_='tme_messages_more')
                if more and 'href' in more.attrs:
                    url = "https://t.me" + more['href']
                    pages += 1
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã –¢–ì –Ω–µ –∑–∞–±–∞–Ω–∏–ª
                    time.sleep(0.5)
                else:
                    break
        except: pass
    
    return list(links)

def extract_ip_port(link):
    try:
        if link.startswith("vmess://"):
            data = json.loads(safe_decode(link[8:]))
            return data.get('add'), int(data.get('port'))
        p = urlparse(link)
        if link.startswith("ss://") and "@" in link:
            part = link.split("@")[-1].split("#")[0].split("/")[0]
            if ":" in part: 
                return part.split(":")[0].replace("[","").replace("]",""), int(part.split(":")[1])
        if p.hostname and p.port: return p.hostname, p.port
    except: pass
    return None, None

async def check_tcp(ip, port):
    try:
        st = time.time()
        conn = asyncio.open_connection(ip, port)
        _, w = await asyncio.wait_for(conn, timeout=1.5)
        lat = int((time.time() - st) * 1000)
        w.close()
        await w.wait_closed()
        return lat
    except: return None

async def vacuum_job():
    """–§–æ–Ω–æ–≤—ã–π –ø—Ä–æ—Ü–µ—Å—Å"""
    while True:
        try:
            # 1. –°–±–æ—Ä (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ, —á—Ç–æ–±—ã –Ω–µ —Ç–æ—Ä–º–æ–∑–∏—Ç—å –±–æ—Ç–∞)
            # –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è, —Ç–∞–∫ –∫–∞–∫ –ª–∏—Å—Ç–∞–µ—Ç –¢–ì
            logging.info("üßπ Vacuum: –ó–∞–ø—É—Å–∫–∞—é —Å–∫–∞–Ω–µ—Ä...")
            all_links = await asyncio.to_thread(scrape_sync)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É (–æ–Ω–∞ —Å–∞–º–∞ –æ—Ç—Å–µ–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã)
            added = db.save_proxy_batch(all_links)
            logging.info(f"üßπ Vacuum: –°–±–æ—Ä –æ–∫–æ–Ω—á–µ–Ω. –ù–æ–≤—ã—Ö: {added}. –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {len(all_links)}")
            
            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ (–±–µ—Ä–µ–º –ø–∞—á–∫—É —Å—Ç–∞—Ä—ã—Ö –∏–ª–∏ –Ω–æ–≤—ã—Ö –Ω–µ–ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Ä—Ü–∏—è–º–∏, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∑–∏—Ç—å —Å–µ—Ç—å
            candidates = db.get_proxies_to_check(limit=MAX_LINKS_CHECK)
            
            if candidates:
                logging.info(f"üß™ Vacuum: –ü—Ä–æ–≤–µ—Ä—è—é {len(candidates)} —Å–µ—Ä–≤–µ—Ä–æ–≤ –Ω–∞ –∂–∏–≤—É—á–µ—Å—Ç—å...")
                sem = asyncio.Semaphore(50) # 50 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
                
                async def verify(url):
                    async with sem:
                        ip, port = extract_ip_port(url)
                        if ip and port:
                            lat = await check_tcp(ip, port)
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º AI (–ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
                            is_ai = 1 if lat and (lat < 150 or "reality" in url.lower()) else 0
                            db.update_proxy_status(url, lat, is_ai, "")
                        else:
                            db.update_proxy_status(url, None, 0, "") # –ù–µ–≤–∞–ª–∏–¥

                await asyncio.gather(*(verify(u) for u in candidates))
                logging.info(f"‚úÖ Vacuum: –ü–∞—á–∫–∞ –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞.")
            
            # –°–ø–∏–º —á–∞—Å –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º —Å–±–æ—Ä–æ–º
            # –ù–æ –ø—Ä–æ–≤–µ—Ä–∫—É –º–æ–∂–Ω–æ –∑–∞–ø—É—Å–∫–∞—Ç—å —á–∞—â–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            logging.info("üí§ Vacuum: –°–ø–ª—é 1 —á–∞—Å...")
            await asyncio.sleep(3600)
            
        except Exception as e:
            logging.error(f"‚ùå Vacuum Error: {e}")
            await asyncio.sleep(60)
